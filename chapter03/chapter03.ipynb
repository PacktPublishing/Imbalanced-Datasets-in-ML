{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Introduction\n",
    "2. Remove Uniformly\n",
    "    1. Random Undersampling\n",
    "    2. Cluster Centroid\n",
    "3. Remove Noisy Observations\n",
    "    1. ENN, RENN, AllKNN\n",
    "    2. Tomek Links\n",
    "    3. Neighborhood Cleaning Rule\n",
    "    4. Instance Hardness\n",
    "4. Remove observations away from the boundary\n",
    "    1. Condensed Nearest Neighbors\n",
    "    2. NearMiss\n",
    "5. One Sided Selection\n",
    "6. Combining Undersampling and oversampling\n",
    "7. Model comparison\n",
    "8. Conclusion\n",
    "9. Question\n",
    "10. Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Two households, both alike in dignity, In fair Verona, where we lay our scene, From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. ~ Opening lines of Romeo and Juliet, Shakespear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a town with two warring communities, viz., Montague and Capulet. They have been enemies of each other for generations. Montague is in the minority, and Capulet is in the majority in the town. Montagues are super rich and powerful. Capulets are not that well off. This creates a complex situation in the town. There are regular riots in the town because of this rivalry. One day, Montague wins the king's favor and conspires to eliminate some Capulets to bring their numbers down. The idea is that if there are fewer Capulets in the town, Montague will no longer be in the minority. King agrees to the plan as he hopes for peace after its execution. We will use this story in this chapter to illustrate various under-sampling algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is not sufficient to over-sample the minority class. With oversampling you can run into problems like overfitting and longer training time. To solve these problems and to approach the issue of class imbalance differently, people have thought of the opposite of oversampling, a.k.a., undersampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling techniques reduce the number of samples in the majority class(es). This method has two obvious advantages over oversampling. The data size remains in check, and there is a lesser chance of overfitting. We are going to discuss the various under-sampling methods in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig 3.1](./fig-3-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed vs cleaning undersampling\n",
    "Undersampling methods can be divided into two categories based on how many data points they remove from the majority class. These categories are Fixed methods and Cleaning methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In fixed methods, the number of samples in the majority class is reduced to a fixed number of samples. Primarily we reduce the number of majority class samples to the size of the minority class. For example, if there are 100 samples in the majority class and 10 samples in the minority class, after applying a fixed method, you will be left with only 10 samples of both classes. We are going to discuss three fixed methods in this chapter. They are Random Undersampling, NearMiss, and Instance Hardness-based undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cleaning methods, the number of samples of the majority class is reduced based on some pre-determined criteria. Once this criterion is met, the algorithm doesn't care about the size of the majority or minority class. All the methods other than the three above methods belong to the cleaning methods category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways the king can eliminate some Capulets. He can eliminate Capulates uniformly from the whole down, removing a few Capulets from all areas of the town. Alternatively, the king can remove Capulates who live near the houses of the Montagues. Lastly, he can remove Capulets, who live far away from the houses of the Montagues. These are the three major approaches used in under-sampling techniques. We either remove the majority samples uniformly, we remove the majority samples near the minority samples, or we remove the majority samples far from the minority samples. We can also combine the last two approaches by removing some nearby and some far-away samples. The following diagram gives the classification of these methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig 3.2](./fig-3-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates the difference between the two criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig 3.3](./fig-3-3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove uniformly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first technique the king might think of is to pick Capulets randomly and remove them from the town. This is a na√Øve approach. It might work, and the king might be able to bring peace to the town. But the king might cause unforeseen damage by picking up some influential Capulets. However, it is an excellent place to start our discussion. This technique is a close cousin of `Random oversampling`. In `Random undersampling`, as the name suggests, we randomly extract observations from the majority class until the classes are balanced. This technique leads to data loss, might harm the underlying structure of the data, and thus perform poorly sometimes.\n",
    "\n",
    "Below is the code sample for using `RandomUnderSampling`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at two diagrams before and after applying RUS.\n",
    "\n",
    "![Fig 2.4](./fig-3-4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second technique the king might follow is to divide the Capulet population into groups based on location. Then keep one Capulet from each group and remove every other Capulet. This method of undersampling is called as **ClusterCentroids** method. If there are N items in the minority class, we create N clusters from the points of the majority class. This can be done using the K-means algorithm. K-means is a clustering algorithm that groups nearby points into different clusters and assigns centroids to each group.\n",
    "\n",
    "In ClusterCentroids technique, we first apply K-means algorithm to all of the majority class data. Then for each cluster, we keep the centroid in the data and remove all other samples. Here, the centroid might not be part of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for using `ClusterCentroids`.\n",
    "```\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_res, y_res = cc.fit_resample(X,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig 3.5](./fig-3-5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for removing noisy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king might decide to look at the friendships and locations of the citizens before removing anyone. In business and politics, a strategy is good if its reverse is a good strategy as well. If the reverse of our strategy is not a good strategy, then there is nothing new in our strategy. We are just stating the obvious. The king might decide to remove the Capulets, who are rich and live near the Montagues. This can bring peace to the city. Let's look at some strategies to do that with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENN, RENN, AllKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king can remove Capulets based on their neighbors. If one or more of the three closest neighbors of a Capulet is Montague, the king will remove him. This technique is called **Edited Nearest Neighbors(ENN)**. **ENN** removes boundary samples to increase the separation between classes. We fit a KNN to the whole dataset. And remove the observations whose neighbors don't belong to the same class. The `imbalanced-learn` library gives us options to decide which classes we would like to resample and what kind of class arrangement the neighbors of the sample should have. \n",
    "\n",
    "The same options for sampling strategy are given for ENN, as discussed in Chapter 1, Introduction.\n",
    "\n",
    "There are two different criteria we can follow for excluding the samples. Either we can choose to exclude samples whose one or more neighbors are not from the same class as themselves. Or we can decide to exclude samples whose majority of the neighbors are not from the same class as themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig 3.5](./fig-3-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for using ENN.\n",
    "```\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors=3, kind_sel='all')\n",
    "X_res, y_res = enn.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king might run **ENN** type of policy for few days. Each day he will calculate the nearest neighbors for the Capulets in the morning. And remove the Capulets who have one or more Montague as neighbors in the evening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Repeated Edited Nearest Neighbors(RENN), we repeat the process followed in ENN until there are no more samples that are removed or the maximum number of cycle count has been reached. This algorithm also removes the noisy data. It is stronger in removing the boundary samples as the algorithm is repeated several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use RENN as follows:\n",
    "```\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "renn = RepeatedEditedNearestNeighbours(sampling_strategy='auto', n_neighbors=3, kind_sel='all', max_iter=200)\n",
    "X_res, y_res = renn.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king might also think of a variant of this approach. On day one, he will calculate one nearest neighbor of every Capulate. If that neighbor is Montegue, the Capulate will be removed. On day two, he will calculate two nearest neighbors of remaining Capulates. If one or more of them are Montegue, the Capulate will be removed. On day three, he will calculate the three nearest neighbors of the remaining Capulate. And remove Capulates with one or more Montegue neighbors. This is similar to **RENN** but the number of neighbors the king is calculating is increasing everyday. \n",
    "\n",
    "This method is called AllKNN. In this method, we repeat **ENN** but with the number of neighbors going from 1 to K. The code of AllKNN is as follows:\n",
    "```\n",
    "from imblearn.under_sampling import AllKNN\n",
    "renn = AllKNN(sampling_strategy='auto', n_neighbors=3, kind_sel='all')\n",
    "X_res, y_res = renn.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1976, Ivan Tomek proposed the idea of Tomek links. Two points are said to form Tomek links if they belong to two different classes and there is no third point with a shorter distance to them than the distance between the two points. The intuition behind Tomek links is that ‚ÄúIf two points are from different classes, they should not be nearest to each other.‚Äù These points are part of the noise and we can eliminate the majority member or both points to reduce noise. This is as if, the king decides to remove Capulets whose best friend is a Montague."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `TomekLinks` API as follows:\n",
    "```\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "tklinks = TomekLinks(sampling_strategy='auto')\n",
    "X_res, y_res = tklinks.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig 3.6](./fig-3-6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood cleaning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of removing Capulets whose one or more nearest neighbors are Montague. The king might decide to look at the nearest neighbors of Montagues and remove the Capulets, who might come up as one of the nearest neighbors for a Montague. In Neighborhood Cleaning Rule(NCR), we apply an ENN algorithm, train a KNN on the remaining data, and then remove all the majority class samples that are the nearest neighbors of the minority samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for using `NeighourhoodCleaningRule`.\n",
    "```\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "ncr = NeighbourhoodCleaningRule(sampling_strategy='auto', n_neighbors=3, threshold_cleaning=0.5)\n",
    "X_res, y_res = ncr.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance hardness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king might ask a minister, \"Which Capulets have mixed well with Montagues?\" The minister based on their knowledge of the town will give a list of those Capulets. And then the king will remove the Capulets whose names are on the list. This method of using another model to identify noisy samples is known as Instance Hardness. In this method, we train a classification model on the data, like, Decision Tree, Random Forest, Linear SVM, etc. In addition to predicting the class of an instance, these classifiers can return their class probabilities. Class probabilities show the confidence the model has in classifying the instances. With Instance Hardness Threshold we remove the majority-class samples that have a low-class probability. These instances are harder to classify and are part of the noise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig 3.7](./fig-3-7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Imbalance-learn` library provides an API for using `InstanceHardnessThreshold`.\n",
    "```\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "nm = InstanceHardnessThreshold(sampling_strategy='auto')\n",
    "X_res, y_res = nm.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for removing easy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse of the strategy to remove the rich and famous Capulets is to remove the poor and weak Capulets. As discussed earlier, the reverse of a good strategy is also a good strategy. This section will discuss the techniques for removing the majority samples far away from the minority samples. Instead of removing the samples from the boundary between the two classes, we use them for training a model. This way, we can train a model that can better discriminate between the classes. As a downside, these models increase the noise in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condensed nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condensed Nearest Neighbors is an algorithm that works as follows:\n",
    "- We add all minority samples to a set and one randomly selected majority sample. Let's call this set C.\n",
    "- We train a KNN model with k = 1.\n",
    "- Now, we repeat the following four steps for each of the remaining majority samples.\n",
    "    - We consider one majority sample; let's call it `e`.\n",
    "    - And try to predict the class of `e` using the KNN.\n",
    "    - If the predicted class matches the original class, we ignore the sample. The intuition is there is not much to learn from `e` as even a 1-NN classifier can learn it.\n",
    "    - Otherwise, we add the sample to our set C and again train the 1-NN on C.\n",
    "\n",
    "This method removes the easy-to-classify samples from the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to use `CondensedNearestNeighbour` looks as follows.\n",
    "```\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour \n",
    "cnn = CondensedNearestNeighbour(random_state=42) \n",
    "X_res, y_res = cnn.fit_resample(X, y) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One sided selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king might decide to remove some rich and many poor Capulets. This way, only the middle-class Capulets will stay in the town. In One sided selection we do just that. This method is the combination of Condensed nearest neighbors and Tomek links. We first resample using a Condensed Nearest Neighbor. Then we remove the Tomek links from the resampled data. This technique makes a compromise between the two types of strategies discussed in the previous two sections. It reduces both noisy and easy-to-identify samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for `OneSidedSelection`. When we don't provide `n_neighbors` parameter, the default value of `1` is taken.\n",
    "```\n",
    "from imblearn.under_sampling import OneSidedSelection \n",
    "oss = OneSidedSelection(random_state=0)\n",
    "X_res, y_res = oss.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining undersampling and oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking if we can combine undersampling techniques with oversampling techniques to produce even better results. \n",
    "The answer is yes and no. We can combine some strategies but not the others. Oversampling methods increase the number of samples of the minority class. But that also usually increases the noise in the data. Some undersampling techniques can help us remove the noise, e.g., ENN, Tomek links, NCR, and Instance hardness. We can combine these methods with SMOTE to produce good results. \n",
    "The combination of SMOTE with ENN and Tomek links has been heavily researched<sup>2</sup>. Also, the `imbalanced-learn` library provides support for both of them: `SMOTEENN` and `SMOTETomek`. \n",
    "\n",
    "[TODO]: explain more on why combining other techniques is not a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "Let's try to see how some popular models behave with the various undersampling techniques we went over. We use four datasets: two synthetic datasets and two real-world datasets. We compare the performance of fourteen undersampling techniques as well as no sampling on logistic regression and XGBoost models. \n",
    "\n",
    "You can find all the related code in the Github repo.\n",
    "\n",
    "![model_perf_undersampling](./model_perf_undersampling.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which method will work best for our data? There is no easy answer to this question. \n",
    "We need to experiment with various techniques and find the most suitable one. The key to note here is to develop an intuition about the working of these methods and have a pipeline that can help us test different techniques.\n",
    "\n",
    "TODO: add something like what chapter-2 did, going over various topics discussed and key learnings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explore the various undersampling APIs available from the `imbalanced-learn` library here: https://imbalanced-learn.org/stable/references/under_sampling.html\n",
    "2. How does the NearMiss undersampling method work? Which class of methods does it belong to? Can you relate this method to the story we have been telling in this chapter? Please use it on the dataset in the chapter.\n",
    "3. Try all the under-sampling methods discussed in this chapter on the `us_crime` dataset from UCI. You can find this dataset in the `fetch_datasets` API of the `imbalanced-learn` library. Find the undersampling method with the highest `f1-score` metric for both `LogisticRegression` and `XGBoost` models.\n",
    "4. Can you identify an undersampling method of your own? (Hint: Think about combining the various approaches to undersampling in new ways.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
